<!doctype html>



  


<html class="theme-next mist use-motion" lang="en">
<head><meta name="generator" content="Hexo 3.9.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">



<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css">


  <meta name="keywords" content="SystemDesign,">








  <link rel="shortcut icon" type="image/x-icon" href="/images/favicon.ico?v=5.1.0">






<meta name="description" content="In this chapter, we focus on web crawler design—an interesting and classic system design interview question. A web crawler is known as a robot or spider. It is widely used by search engines to discove">
<meta name="keywords" content="SystemDesign">
<meta property="og:type" content="article">
<meta property="og:title" content="Design a Web Crawler">
<meta property="og:url" content="https://necusjz.github.io/p/384409989.html">
<meta property="og:site_name" content="Ethan&#39;s Blog">
<meta property="og:description" content="In this chapter, we focus on web crawler design—an interesting and classic system design interview question. A web crawler is known as a robot or spider. It is widely used by search engines to discove">
<meta property="og:locale" content="en">
<meta property="og:image" content="https://raw.githubusercontent.com/necusjz/p/master/SystemDesign/bytebytego/10/00.webp">
<meta property="og:image" content="https://raw.githubusercontent.com/necusjz/p/master/SystemDesign/bytebytego/10/01.svg">
<meta property="og:image" content="https://raw.githubusercontent.com/necusjz/p/master/SystemDesign/bytebytego/10/02.svg">
<meta property="og:image" content="https://raw.githubusercontent.com/necusjz/p/master/SystemDesign/bytebytego/10/03.svg">
<meta property="og:image" content="https://raw.githubusercontent.com/necusjz/p/master/SystemDesign/bytebytego/10/04.svg">
<meta property="og:image" content="https://raw.githubusercontent.com/necusjz/p/master/SystemDesign/bytebytego/10/06.svg">
<meta property="og:image" content="https://raw.githubusercontent.com/necusjz/p/master/SystemDesign/bytebytego/10/05.svg">
<meta property="og:image" content="https://raw.githubusercontent.com/necusjz/p/master/SystemDesign/bytebytego/10/07.svg">
<meta property="og:image" content="https://raw.githubusercontent.com/necusjz/p/master/SystemDesign/bytebytego/10/08.svg">
<meta property="og:image" content="https://raw.githubusercontent.com/necusjz/p/master/SystemDesign/bytebytego/10/09.svg">
<meta property="og:updated_time" content="2025-04-19T14:28:27.916Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Design a Web Crawler">
<meta name="twitter:description" content="In this chapter, we focus on web crawler design—an interesting and classic system design interview question. A web crawler is known as a robot or spider. It is widely used by search engines to discove">
<meta name="twitter:image" content="https://raw.githubusercontent.com/necusjz/p/master/SystemDesign/bytebytego/10/00.webp">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    sidebar: {"position":"right","display":"post","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://necusjz.github.io/p/384409989.html">





  <title> Design a Web Crawler | Ethan's Blog </title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  














  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container one-collumn sidebar-position-right page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Ethan's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            Tags
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocapitalize="off" autocomplete="off" autocorrect="off" placeholder="Searching..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://necusjz.github.io/p/384409989.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="necusjz">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ethan's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                Design a Web Crawler
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2023-12-21T23:23:05+11:00">
                2023-12-21
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>In this chapter, we focus on web crawler design—an interesting and classic system design interview question.</p>
<p>A web crawler is known as a robot or spider. It is widely used by search engines to discover new or updated content on the web. Content can be a web page, an image, a video, a PDF file, etc. A web crawler starts by collecting a few web pages and then follows links on those pages to collect new content. Figure 1 shows a visual example of the crawl process.<br><img src="https://raw.githubusercontent.com/necusjz/p/master/SystemDesign/bytebytego/10/00.webp" alt></p>
<a id="more"></a>
<p>A crawler is used for many purposes:</p>
<ul>
<li>Search Engine Indexing: This is the most common use case. A crawler collects web pages to create a local index for search engines. For example, Googlebot is the web crawler behind the Google search engine.</li>
<li>Web Archiving: This is the process of collecting information from the web to preserve data for future uses. For instance, many national libraries run crawlers to archive websites. Notable examples are Library of Congress [1] and EU Web Archive [2].</li>
<li>Web Mining: The explosive growth of the web presents an unprecedented opportunity for data mining. Web mining helps to discover useful knowledge from the internet. For example, top financial firms use crawlers to download shareholder meetings and annual reports to learn key company initiatives.</li>
<li>Web Monitoring: The crawlers help to monitor copyright and trademark infringements over the internet. For example, Digimarc [3] utilizes crawlers to discover pirated works and reports.</li>
</ul>
<p>The complexity of developing a web crawler depends on the scale we intend to support. It could be either a small school project, which takes only a few hours to complete or a gigantic project that requires continuous improvement from a dedicated engineering team. Thus, we will explore the scale and features to support below.</p>
<h2 id="Step-1-Understand-the-problem-and-establish-design-scope"><a href="#Step-1-Understand-the-problem-and-establish-design-scope" class="headerlink" title="Step 1 - Understand the problem and establish design scope"></a>Step 1 - Understand the problem and establish design scope</h2><p>The basic algorithm of a web crawler is simple:</p>
<ol>
<li>Given a set of URLs, download all the web pages addressed by the URLs.</li>
<li>Extract URLs from these web pages.</li>
<li>Add new URLs to the list of URLs to be downloaded. Repeat these 3 steps.</li>
</ol>
<p>Does a web crawler work truly as simple as this basic algorithm? Not exactly. Designing a vastly scalable web crawler is an extremely complex task. It is unlikely for anyone to design a massive web crawler within the interview duration. Before jumping into the design, we must ask questions to understand the requirements and establish design scope:<br>Candidate: What is the main purpose of the crawler? Is it used for search engine indexing, data mining, or something else?<br>Interviewer: Search engine indexing.<br>C: How many web pages does the web crawler collect per month?<br>I: 1 billion pages.<br>C: What content types are included? HTML only or other content types such as PDFs and images as well?<br>I: HTML only.<br>C: Shall we consider newly added or edited web pages?<br>I: Yes, we should consider the newly added or edited web pages.<br>C: Do we need to store HTML pages crawled from the web?<br>I: Yes, up to 5 years.<br>C: How do we handle web pages with duplicate content?<br>I: Pages with duplicate content should be ignored.</p>
<p>Above are some of the sample questions that you can ask your interviewer. It is important to understand the requirements and clarify ambiguities. Even if you are asked to design a straightforward product like a web crawler, you and your interviewer might not have the same assumptions.</p>
<p>Beside functionalities to clarify with your interviewer, it is also important to note down the following characteristics of a good web crawler:</p>
<ul>
<li>Scalability: The web is very large. There are billions of web pages out there. Web crawling should be extremely efficient using parallelization.</li>
<li>Politeness: The crawler should not make too many requests to a website within a short time interval.</li>
<li>Robustness: The web is full of traps. Bad HTML, unresponsive servers, crashes, malicious links, etc. are all common. The crawler must handle all those edge cases.</li>
<li>Extensibility: The system is flexible so that minimal changes are needed to support new content types. For example, if we want to crawl image files in the future, we should not need to redesign the entire system.</li>
</ul>
<h3 id="Back-of-the-envelope-estimation"><a href="#Back-of-the-envelope-estimation" class="headerlink" title="Back-of-the-envelope estimation"></a>Back-of-the-envelope estimation</h3><p>The following estimations are based on many assumptions, and it is important to communicate with the interviewer to be on the same page:</p>
<ul>
<li>Assume 1 billion web pages are downloaded every month.</li>
<li>QPS: 1,000,000,000 / 30 days / 24 hours / 3600 seconds = ~400 pages per second.</li>
<li>Peak QPS = QPS * 2 = 800</li>
<li>Assume the average web page size is 500k.</li>
<li>1-billion-page x 500k = 500 TB storage per month. If you are unclear about digital storage units, go through “Power of 2” section in the “Back-of-the-envelope Estimation” chapter again.</li>
<li>Assuming data are stored for five years, 500 TB * 12 months * 5 years = 30 PB. A 30 PB storage is needed to store five-year content.</li>
</ul>
<h2 id="Step-2-Propose-high-level-design-and-get-buy-in"><a href="#Step-2-Propose-high-level-design-and-get-buy-in" class="headerlink" title="Step 2 - Propose high-level design and get buy-in"></a>Step 2 - Propose high-level design and get buy-in</h2><p>Once the requirements are clear, we move on to the high-level design. Inspired by previous studies on web crawling [4] [5], we propose a high-level design as shown in Figure 2.<br><img src="https://raw.githubusercontent.com/necusjz/p/master/SystemDesign/bytebytego/10/01.svg" alt></p>
<p>First, we explore each design component to understand their functionalities. Then, we examine the crawler workflow step-by-step.</p>
<h3 id="Seed-URLs"><a href="#Seed-URLs" class="headerlink" title="Seed URLs"></a>Seed URLs</h3><p>A web crawler uses seed URLs as a starting point for the crawl process. For example, to crawl all web pages from a university’s website, an intuitive way to select seed URLs is to use the university’s domain name.</p>
<p>To crawl the entire web, we need to be creative in selecting seed URLs. A good seed URL serves as a good starting point that a crawler can utilize to traverse as many links as possible. The general strategy is to divide the entire URL space into smaller ones. The first proposed approach is based on locality as different countries may have different popular websites. Another way is to choose seed URLs based on topics; for example, we can divide URL space into shopping, sports, healthcare, etc. Seed URL selection is an open-ended question. You are not expected to give the perfect answer. Just think out loud.</p>
<h3 id="URL-Frontier"><a href="#URL-Frontier" class="headerlink" title="URL Frontier"></a>URL Frontier</h3><p>Most modern web crawlers split the crawl state into two: <em>to_be_downloaded</em> and <em>already_downloaded</em>. The component that stores URLs to be downloaded is called the URL Frontier. You can refer to this as a FIFO queue. For detailed information about the URL Frontier, refer to the deep dive.</p>
<h3 id="HTML-Downloader"><a href="#HTML-Downloader" class="headerlink" title="HTML Downloader"></a>HTML Downloader</h3><p>The HTML Downloader downloads web pages from the internet. Those URLs are provided by the URL Frontier.</p>
<h3 id="DNS-Resolver"><a href="#DNS-Resolver" class="headerlink" title="DNS Resolver"></a>DNS Resolver</h3><p>To download a web page, a URL must be translated into an IP address. The HTML Downloader calls the DNS Resolver to get the corresponding IP address for the URL. For instance, URL <a href="http://www.wikipedia.org" target="_blank" rel="noopener">www.wikipedia.org</a> is converted to IP address 198.35.26.96 as of 03/05/2019.</p>
<h3 id="Content-Parser"><a href="#Content-Parser" class="headerlink" title="Content Parser"></a>Content Parser</h3><p>After a web page is downloaded, it must be parsed and validated because malformed web pages could provoke problems and waste storage space. Implementing a content parser in a crawl server will slow down the crawling process. Thus, the content parser is a separate component.</p>
<h3 id="Content-Seen"><a href="#Content-Seen" class="headerlink" title="Content Seen?"></a>Content Seen?</h3><p>Online research [6] reveals that 29% of the web pages are duplicated contents, which may cause the same content to be stored multiple times. We introduce the “Content Seen?” data structure to eliminate data redundancy and shorten processing time. It helps to detect new content previously stored in the system. To compare two HTML documents, we can compare them character by character. However, this method is slow and time-consuming, especially when billions of web pages are involved. An efficient way to accomplish this task is to compare the hash values of the two web pages [7].</p>
<h3 id="Content-Storage"><a href="#Content-Storage" class="headerlink" title="Content Storage"></a>Content Storage</h3><p>It is a storage system for storing HTML content. The choice of storage system depends on factors such as data type, data size, access frequency, life span, etc. Both disk and memory are used:</p>
<ul>
<li>Most of the content is stored on disk because the dataset is too big to fit in memory.</li>
<li>Popular content is kept in memory to reduce latency.</li>
</ul>
<h3 id="URL-Extractor"><a href="#URL-Extractor" class="headerlink" title="URL Extractor"></a>URL Extractor</h3><p>URL Extractor parses and extracts links from HTML pages. Figure 3 shows an example of a link extraction process. Relative paths are converted to absolute URLs by adding the <a href="https://en.wikipedia.org" target="_blank" rel="noopener">https://en.wikipedia.org</a> prefix.<br><img src="https://raw.githubusercontent.com/necusjz/p/master/SystemDesign/bytebytego/10/02.svg" alt></p>
<h3 id="URL-Filter"><a href="#URL-Filter" class="headerlink" title="URL Filter"></a>URL Filter</h3><p>The URL filter excludes certain content types, file extensions, error links and URLs in “blacklisted” sites.</p>
<h3 id="URL-Seen"><a href="#URL-Seen" class="headerlink" title="URL Seen?"></a>URL Seen?</h3><p>“URL Seen?” is a data structure that keeps track of URLs that are visited before or already in the Frontier. “URL Seen?” helps to avoid adding the same URL multiple times as this can increase server load and cause potential infinite loops.</p>
<p>Bloom filter and hash table are common techniques to implement the “URL Seen?” component. We will not cover the detailed implementation of the bloom filter and hash table here. For more information, refer to the reference materials [4] [8].</p>
<h3 id="URL-Storage"><a href="#URL-Storage" class="headerlink" title="URL Storage"></a>URL Storage</h3><p>URL Storage stores already visited URLs.</p>
<p>So far, we have discussed every system component. Next, we put them together to explain the workflow. To better explain the workflow step-by-step, sequence numbers are added in the design diagram as shown in Figure 4:<br><img src="https://raw.githubusercontent.com/necusjz/p/master/SystemDesign/bytebytego/10/03.svg" alt></p>
<ol>
<li>Add seed URLs to the URL Frontier.</li>
<li>HTML Downloader fetches a list of URLs from URL Frontier.</li>
<li>HTML Downloader gets IP addresses of URLs from DNS Resolver and starts downloading.</li>
<li>Content Parser parses HTML pages and checks if pages are malformed.</li>
<li>After content is parsed and validated, it is passed to the “Content Seen?” component.</li>
<li>“Content Seen?” component checks if a HTML page is already in the storage:<ul>
<li>If it is in the storage, this means the same content in a different URL has already been processed. In this case, the HTML page is discarded.</li>
<li>If it is not in the storage, the system has not processed the same content before. The content is passed to Link Extractor.</li>
</ul>
</li>
<li>Link Extractor extracts links from HTML pages.</li>
<li>Extracted links are passed to the URL filter.</li>
<li>After links are filtered, they are passed to the “URL Seen?” component.</li>
<li>“URL Seen?” component checks if a URL is already in the storage, if yes, it is processed before, and nothing needs to be done.</li>
<li>If a URL has not been processed before, it is added to the URL Frontier.</li>
</ol>
<h2 id="Step-3-Design-deep-dive"><a href="#Step-3-Design-deep-dive" class="headerlink" title="Step 3 - Design deep dive"></a>Step 3 - Design deep dive</h2><p>Up until now, we have discussed the high-level design. Next, we will discuss the most important building components and techniques in depth:</p>
<ul>
<li>DFS vs. BFS</li>
<li>URL Frontier</li>
<li>HTML Downloader</li>
<li>Robustness</li>
<li>Extensibility</li>
<li>Detect and avoid problematic content</li>
</ul>
<h3 id="DFS-vs-BFS"><a href="#DFS-vs-BFS" class="headerlink" title="DFS vs. BFS"></a>DFS vs. BFS</h3><p>You can think of the web as a directed graph where web pages serve as nodes and hyperlinks (URLs) as edges. The crawl process can be seen as traversing a directed graph from one web page to others. Two common graph traversal algorithms are DFS and BFS. However, DFS is usually not a good choice because the depth of DFS can be very deep.</p>
<p>BFS is commonly used by web crawlers and is implemented by a FIFO queue. In a FIFO queue, URLs are dequeued in the order they are enqueued. However, this implementation has two problems:</p>
<ul>
<li>Most links from the same web page are linked back to the same host. In Figure 5, all the links in wikipedia.com are internal links, making the crawler busy processing URLs from the same host (wikipedia.com). When the crawler tries to download web pages in parallel, Wikipedia servers will be flooded with requests. This is considered as “impolite”.<br><img src="https://raw.githubusercontent.com/necusjz/p/master/SystemDesign/bytebytego/10/04.svg" alt></li>
<li>Standard BFS does not take the priority of a URL into consideration. The web is large and not every page has the same level of quality and importance. Therefore, we may want to prioritize URLs according to their page ranks, web traffic, update frequency, etc.</li>
</ul>
<h3 id="URL-Frontier-1"><a href="#URL-Frontier-1" class="headerlink" title="URL Frontier"></a>URL Frontier</h3><p>URL Frontier helps to address these problems. A URL Frontier is a data structure that stores URLs to be downloaded. The URL Frontier is an important component to ensure politeness, URL prioritization, and freshness. A few noteworthy papers on URL Frontier are mentioned in the reference materials [5] [9]. The findings from these papers are as follows.</p>
<h4 id="Priority"><a href="#Priority" class="headerlink" title="Priority"></a>Priority</h4><p>A random post from a discussion forum about Apple products carries very different weight than posts on the Apple home page. Even though they both have the “Apple” keyword, it is sensible for a crawler to crawl the Apple home page first.</p>
<p>We prioritize URLs based on usefulness, which can be measured by PageRank [10], website traffic, update frequency, etc. “Prioritizer” is the component that handles URL prioritization. Refer to the reference materials [5] [10] for in-depth information about this concept.</p>
<p>Figure 6 shows the design that manages URL priority.<br><img src="https://raw.githubusercontent.com/necusjz/p/master/SystemDesign/bytebytego/10/06.svg" alt></p>
<ul>
<li>Prioritizer: It takes URLs as input and computes the priorities.</li>
<li>FIFO queues f1 to fn: Each queue has an assigned priority. Queues with high priority are selected with higher probability.</li>
<li>Queue selector: Randomly choose a queue with a bias towards queues with higher priority.</li>
</ul>
<h4 id="Politeness"><a href="#Politeness" class="headerlink" title="Politeness"></a>Politeness</h4><p>Generally, a web crawler should avoid sending too many requests to the same hosting server within a short period. Sending too many requests is considered as “impolite” or even treated as Denial-of-Service (DoS) attack. For example, without any constraint, the crawler can send thousands of requests every second to the same website. This can overwhelm the web servers.</p>
<p>The general idea of enforcing politeness is to download one page at a time from the same host. A delay can be added between two download tasks. The politeness constraint is implemented by maintain a mapping from website hostnames to download (worker) threads. Each downloader thread has a separate FIFO queue and only downloads URLs obtained from that queue. Figure 7 shows the design that manages politeness:<br><img src="https://raw.githubusercontent.com/necusjz/p/master/SystemDesign/bytebytego/10/05.svg" alt></p>
<ul>
<li>Queue router: It ensures that each queue (b1, b2, …, bn) only contains URLs from the same host.</li>
<li>Mapping table: It maps each host to a queue:<table>
<thead>
<tr>
<th align="left">Host</th>
<th align="left">Queue</th>
</tr>
</thead>
<tbody><tr>
<td align="left">wikipedia.com</td>
<td align="left">b1</td>
</tr>
<tr>
<td align="left">apple.com</td>
<td align="left">b2</td>
</tr>
<tr>
<td align="left">…</td>
<td align="left">…</td>
</tr>
<tr>
<td align="left">nike.com</td>
<td align="left">bn</td>
</tr>
</tbody></table>
</li>
<li>FIFO queues b1 to bn: Each queue contains URLs from the same host.</li>
<li>Queue selector: Each worker thread is mapped to a FIFO queue, and it only downloads URLs from that queue. The queue selection logic is done by the Queue selector.</li>
<li>Worker threads 1 to N. A worker thread downloads web pages one by one from the same host. A delay can be added between two download tasks.</li>
</ul>
<p>Figure 8 presents the URL Frontier design, and it contains two modules:</p>
<ul>
<li>Front queues: Manage prioritization.</li>
<li>Back queues: Manage politeness.</li>
</ul>
<p><img src="https://raw.githubusercontent.com/necusjz/p/master/SystemDesign/bytebytego/10/07.svg" alt></p>
<h4 id="Freshness"><a href="#Freshness" class="headerlink" title="Freshness"></a>Freshness</h4><p>Web pages are constantly being added, deleted, and edited. A web crawler must periodically recrawl downloaded pages to keep our dataset fresh. Recrawl all the URLs is time-consuming and resource intensive. Few strategies to optimize freshness are listed as follows:</p>
<ul>
<li>Recrawl based on web pages’ update history.</li>
<li>Prioritize URLs and recrawl important pages first and more frequently.</li>
</ul>
<h4 id="Storage-for-URL-Frontier"><a href="#Storage-for-URL-Frontier" class="headerlink" title="Storage for URL Frontier"></a>Storage for URL Frontier</h4><p>In real-world crawl for search engines, the number of URLs in the frontier could be hundreds of millions [4]. Putting everything in memory is neither durable nor scalable. Keeping everything in the disk is undesirable neither because the disk is slow; and it can easily become a bottleneck for the crawl.</p>
<p>We adopted a hybrid approach. The majority of URLs are stored on disk, so the storage space is not a problem. To reduce the cost of reading from the disk and writing to the disk, we maintain buffers in memory for enqueue/dequeue operations. Data in the buffer is periodically written to the disk.</p>
<h3 id="HTML-Downloader-1"><a href="#HTML-Downloader-1" class="headerlink" title="HTML Downloader"></a>HTML Downloader</h3><p>The HTML Downloader downloads web pages from the internet using the HTTP protocol. Before discussing the HTML Downloader, we look at Robots Exclusion Protocol first.</p>
<p><em>robots.txt</em>, called Robots Exclusion Protocol, is a standard used by websites to communicate with crawlers. It specifies what pages crawlers are allowed to download. Before attempting to crawl a website, a crawler should check its corresponding <em>robots.txt</em> first and follow its rules.</p>
<p>To avoid repeat downloads of <em>robots.txt</em> file, we cache the results of the file. The file is downloaded and saved to cache periodically. Here is a piece of <em>robots.txt</em> file taken from <a href="https://www.amazon.com/robots.txt" target="_blank" rel="noopener">https://www.amazon.com/robots.txt</a>. Some of the directories like creatorhub are disallowed for Googlebot:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">User-agent: Googlebot</span><br><span class="line">Disallow: /creatorhub/\*</span><br><span class="line">Disallow: /rss/people/\*/reviews</span><br><span class="line">Disallow: /gp/pdp/rss/\*/reviews</span><br><span class="line">Disallow: /gp/cdp/member-reviews/</span><br><span class="line">Disallow: /gp/aw/cr/</span><br></pre></td></tr></table></figure>

<p>Besides <em>robots.txt</em>, performance optimization is another important concept we will cover for the HTML Downloader.</p>
<h4 id="Performance-optimization"><a href="#Performance-optimization" class="headerlink" title="Performance optimization"></a>Performance optimization</h4><p>Below is a list of performance optimizations for HTML Downloader.</p>
<h5 id="Distributed-crawl"><a href="#Distributed-crawl" class="headerlink" title="Distributed crawl"></a>Distributed crawl</h5><p>To achieve high performance, crawl jobs are distributed into multiple servers, and each server runs multiple threads. The URL space is partitioned into smaller pieces; so, each downloader is responsible for a subset of the URLs. Figure 9 shows an example of a distributed crawl.<br><img src="https://raw.githubusercontent.com/necusjz/p/master/SystemDesign/bytebytego/10/08.svg" alt></p>
<h5 id="Cache-DNS-Resolver"><a href="#Cache-DNS-Resolver" class="headerlink" title="Cache DNS Resolver"></a>Cache DNS Resolver</h5><p>DNS Resolver is a bottleneck for crawlers because DNS requests might take time due to the synchronous nature of many DNS interfaces. DNS response time ranges from 10ms to 200ms. Once a request to DNS is carried out by a crawler thread, other threads are blocked until the first request is completed. Maintaining our DNS cache to avoid calling DNS frequently is an effective technique for speed optimization. Our DNS cache keeps the domain name to IP address mapping and is updated periodically by cron jobs.</p>
<h5 id="Locality"><a href="#Locality" class="headerlink" title="Locality"></a>Locality</h5><p>Distribute crawl servers geographically. When crawl servers are closer to website hosts, crawlers experience faster download time. Design locality applies to most of the system components: crawl servers, cache, queue, storage, etc.</p>
<h5 id="Short-timeout"><a href="#Short-timeout" class="headerlink" title="Short timeout"></a>Short timeout</h5><p>Some web servers respond slowly or may not respond at all. To avoid long wait time, a maximal wait time is specified. If a host does not respond within a predefined time, the crawler will stop the job and crawl some other pages.</p>
<h3 id="Robustness"><a href="#Robustness" class="headerlink" title="Robustness"></a>Robustness</h3><p>Besides performance optimization, robustness is also an important consideration. We present a few approaches to improve the system robustness:</p>
<ul>
<li>Consistent hashing: This helps to distribute loads among downloaders. A new downloader server can be added or removed using consistent hashing. Refer to the “Design consistent hashing” chapter for more details.</li>
<li>Save crawl states and data: To guard against failures, crawl states and data are written to a storage system. A disrupted crawl can be restarted easily by loading saved states and data.</li>
<li>Exception handling: Errors are inevitable and common in a large-scale system. The crawler must handle exceptions gracefully without crashing the system.</li>
<li>Data validation: This is an important measure to prevent system errors.</li>
</ul>
<h3 id="Extensibility"><a href="#Extensibility" class="headerlink" title="Extensibility"></a>Extensibility</h3><p>As almost every system evolves, one of the design goals is to make the system flexible enough to support new content types. The crawler can be extended by plugging in new modules. Figure 10 shows how to add new modules:<br><img src="https://raw.githubusercontent.com/necusjz/p/master/SystemDesign/bytebytego/10/09.svg" alt></p>
<ul>
<li>PNG Downloader module is plugged-in to download PNG files.</li>
<li>Web Monitor module is added to monitor the web and prevent copyright and trademark infringements.</li>
</ul>
<h3 id="Detect-and-avoid-problematic-content"><a href="#Detect-and-avoid-problematic-content" class="headerlink" title="Detect and avoid problematic content"></a>Detect and avoid problematic content</h3><p>This section discusses the detection and prevention of redundant, meaningless, or harmful content.</p>
<h4 id="Redundant-content"><a href="#Redundant-content" class="headerlink" title="Redundant content"></a>Redundant content</h4><p>As discussed previously, nearly 30% of the web pages are duplicates. Hashes or checksums help to detect duplication [11].</p>
<h4 id="Spider-traps"><a href="#Spider-traps" class="headerlink" title="Spider traps"></a>Spider traps</h4><p>A spider trap is a web page that causes a crawler in an infinite loop. For instance, an infinite deep directory structure is listed as follows: <a href="http://www.spidertrapexample.com/foo/bar/foo/bar/foo/bar/" target="_blank" rel="noopener">http://www.spidertrapexample.com/foo/bar/foo/bar/foo/bar/</a>…</p>
<p>Such spider traps can be avoided by setting a maximal length for URLs. However, no one-size-fits-all solution exists to detect spider traps. Websites containing spider traps are easy to identify due to an unusually large number of web pages discovered on such websites. It is hard to develop automatic algorithms to avoid spider traps; however, a user can manually verify and identify a spider trap, and either exclude those websites from the crawler or apply some customized URL filters.</p>
<h4 id="Data-noise"><a href="#Data-noise" class="headerlink" title="Data noise"></a>Data noise</h4><p>Some of the contents have little or no value, such as advertisements, code snippets, spam URLs, etc. Those contents are not useful for crawlers and should be excluded if possible.</p>
<h2 id="Step-4-Wrap-up"><a href="#Step-4-Wrap-up" class="headerlink" title="Step 4 - Wrap up"></a>Step 4 - Wrap up</h2><p>In this chapter, we first discussed the characteristics of a good crawler: scalability, politeness, robustness, and extensibility. Then, we proposed a design and discussed key components. Building a scalable web crawler is not a trivial task because the web is enormously large and full of traps. Even though we have covered many topics, we still miss many relevant talking points:</p>
<ul>
<li>Server-side rendering: Numerous websites use scripts like JavaScript, Ajax, etc. to generate links on the fly. If we download and parse web pages directly, we will not be able to retrieve dynamically generated links. To solve this problem, we perform server-side rendering (also called dynamic rendering) first before parsing a page [12].</li>
<li>Filter out unwanted pages: With finite storage capacity and crawl resources, an anti-spam component is beneficial in filtering out low quality and spam pages [13] [14].</li>
<li>Database replication and sharding: Techniques like replication and sharding are used to improve the data layer availability, scalability, and reliability.</li>
<li>Horizontal scaling: For large-scale crawl, hundreds or even thousands of servers are needed to perform download tasks. The key is to keep servers stateless.</li>
<li>Availability, consistency, and reliability: These concepts are at the core of any large system’s success.</li>
<li>Analytics: Collecting and analyzing data are important parts of any system because data is key ingredient for fine-tuning.</li>
</ul>
<p>Congratulations on getting this far! Now give yourself a pat on the back. Good job!</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] Library of Congress: <a href="https://www.loc.gov/websites/" target="_blank" rel="noopener">https://www.loc.gov/websites/</a><br>[2] EU Web Archive: <a href="http://data.europa.eu/webarchive" target="_blank" rel="noopener">http://data.europa.eu/webarchive</a><br>[3] Digimarc: <a href="https://www.digimarc.com/products/digimarc-services/piracy-intelligence" target="_blank" rel="noopener">https://www.digimarc.com/products/digimarc-services/piracy-intelligence</a><br>[4] Heydon A., Najork M. Mercator: A scalable, extensible web crawler World Wide Web, 2 (4) (1999), pp. 219-229<br>[5] By Christopher Olston, Marc Najork: Web Crawling.<br><a href="http://infolab.stanford.edu/~olston/publications/crawling_survey.pdf" target="_blank" rel="noopener">http://infolab.stanford.edu/~olston/publications/crawling_survey.pdf</a><br>[6] 29% Of Sites Face Duplicate Content Issues: <a href="https://tinyurl.com/y6tmh55y" target="_blank" rel="noopener">https://tinyurl.com/y6tmh55y</a><br>[7] Rabin M.O., et al. Fingerprinting by random polynomials Center for Research in Computing Techn., Aiken Computation Laboratory, Univ. (1981)<br>[8] B. H. Bloom, “Space/time trade-offs in hash coding with allowable errors,” Communications of the ACM, vol. 13, no. 7, pp. 422–426, 1970.<br>[9] Donald J. Patterson, Web Crawling:<br><a href="https://www.ics.uci.edu/~lopes/teaching/cs221W12/slides/Lecture05.pdf" target="_blank" rel="noopener">https://www.ics.uci.edu/~lopes/teaching/cs221W12/slides/Lecture05.pdf</a><br>[10] L. Page, S. Brin, R. Motwani, and T. Winograd, “The PageRank citation ranking: Bringing order to the web,” Technical Report, Stanford University, 1998.<br>[11] Burton Bloom. Space/time trade-offs in hash coding with allowable errors. Communications of the ACM, 13(7), pages 422–426, July 1970.<br>[12] Google Dynamic Rendering: <a href="https://developers.google.com/search/docs/guides/dynamic-rendering" target="_blank" rel="noopener">https://developers.google.com/search/docs/guides/dynamic-rendering</a><br>[13] T. Urvoy, T. Lavergne, and P. Filoche, “Tracking web spam with hidden style similarity,” in Proceedings of the 2nd International Workshop on Adversarial Information Retrieval on the Web, 2006.<br>[14] H.-T. Lee, D. Leonard, X. Wang, and D. Loguinov, “IRLbot: Scaling to 6 billion pages and beyond,” in Proceedings of the 17th International World Wide Web Conference, 2008.</p>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>


    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/SystemDesign/" rel="tag"># SystemDesign</a>
          
        </div>
      

      
        
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/p/3500128686.html" rel="next" title="Design a URL Shortener">
                <i class="fa fa-chevron-left"></i> Design a URL Shortener
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/p/3940693378.html" rel="prev" title="Design a Notification System">
                Design a Notification System <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>

          
          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            Overview
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image" src="/images/avatar.png" alt="necusjz">
          <p class="site-author-name" itemprop="name">necusjz</p>
           
              <p class="site-description motion-element" itemprop="description"></p>
          
        </div>
        <nav class="site-state motion-element">
        
          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">279</span>
                <span class="site-state-item-name">posts</span>
              </a>
            </div>
          

          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">16</span>
                <span class="site-state-item-name">tags</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Step-1-Understand-the-problem-and-establish-design-scope"><span class="nav-number">1.</span> <span class="nav-text">Step 1 - Understand the problem and establish design scope</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Back-of-the-envelope-estimation"><span class="nav-number">1.1.</span> <span class="nav-text">Back-of-the-envelope estimation</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Step-2-Propose-high-level-design-and-get-buy-in"><span class="nav-number">2.</span> <span class="nav-text">Step 2 - Propose high-level design and get buy-in</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Seed-URLs"><span class="nav-number">2.1.</span> <span class="nav-text">Seed URLs</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#URL-Frontier"><span class="nav-number">2.2.</span> <span class="nav-text">URL Frontier</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#HTML-Downloader"><span class="nav-number">2.3.</span> <span class="nav-text">HTML Downloader</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#DNS-Resolver"><span class="nav-number">2.4.</span> <span class="nav-text">DNS Resolver</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Content-Parser"><span class="nav-number">2.5.</span> <span class="nav-text">Content Parser</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Content-Seen"><span class="nav-number">2.6.</span> <span class="nav-text">Content Seen?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Content-Storage"><span class="nav-number">2.7.</span> <span class="nav-text">Content Storage</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#URL-Extractor"><span class="nav-number">2.8.</span> <span class="nav-text">URL Extractor</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#URL-Filter"><span class="nav-number">2.9.</span> <span class="nav-text">URL Filter</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#URL-Seen"><span class="nav-number">2.10.</span> <span class="nav-text">URL Seen?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#URL-Storage"><span class="nav-number">2.11.</span> <span class="nav-text">URL Storage</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Step-3-Design-deep-dive"><span class="nav-number">3.</span> <span class="nav-text">Step 3 - Design deep dive</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#DFS-vs-BFS"><span class="nav-number">3.1.</span> <span class="nav-text">DFS vs. BFS</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#URL-Frontier-1"><span class="nav-number">3.2.</span> <span class="nav-text">URL Frontier</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Priority"><span class="nav-number">3.2.1.</span> <span class="nav-text">Priority</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Politeness"><span class="nav-number">3.2.2.</span> <span class="nav-text">Politeness</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Freshness"><span class="nav-number">3.2.3.</span> <span class="nav-text">Freshness</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Storage-for-URL-Frontier"><span class="nav-number">3.2.4.</span> <span class="nav-text">Storage for URL Frontier</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#HTML-Downloader-1"><span class="nav-number">3.3.</span> <span class="nav-text">HTML Downloader</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Performance-optimization"><span class="nav-number">3.3.1.</span> <span class="nav-text">Performance optimization</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Distributed-crawl"><span class="nav-number">3.3.1.1.</span> <span class="nav-text">Distributed crawl</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Cache-DNS-Resolver"><span class="nav-number">3.3.1.2.</span> <span class="nav-text">Cache DNS Resolver</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Locality"><span class="nav-number">3.3.1.3.</span> <span class="nav-text">Locality</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Short-timeout"><span class="nav-number">3.3.1.4.</span> <span class="nav-text">Short timeout</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Robustness"><span class="nav-number">3.4.</span> <span class="nav-text">Robustness</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Extensibility"><span class="nav-number">3.5.</span> <span class="nav-text">Extensibility</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Detect-and-avoid-problematic-content"><span class="nav-number">3.6.</span> <span class="nav-text">Detect and avoid problematic content</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Redundant-content"><span class="nav-number">3.6.1.</span> <span class="nav-text">Redundant content</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Spider-traps"><span class="nav-number">3.6.2.</span> <span class="nav-text">Spider traps</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Data-noise"><span class="nav-number">3.6.3.</span> <span class="nav-text">Data noise</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Step-4-Wrap-up"><span class="nav-number">4.</span> <span class="nav-text">Step 4 - Wrap up</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References"><span class="nav-number">5.</span> <span class="nav-text">References</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">
  
  &copy;  2016 - 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">necusjz</span>
</div>


<div class="powered-by">
  Powered by <a class="theme-link" href="https://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    
    
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  




  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  



  




	





  





  





  



  
  
  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length == 0) {
      search_path = "search.xml";
    }
    var path = "/" + search_path;
    // monitor main search box;

    function proceedsearch() {
      $("body").append('<div class="local-search-pop-overlay">').css('overflow', 'hidden');
      $('.popup').toggle();
    }
    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';
      $.ajax({
        url: path,
        dataType: "xml",
        async: true,
        success: function( xmlResponse ) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = $( "entry", xmlResponse ).map(function() {
            return {
              title: $( "title", this ).text(),
              content: $("content",this).text(),
              url: $( "url" , this).text()
            };
          }).get();
          var $input = document.getElementById(search_id);
          var $resultContent = document.getElementById(content_id);
          $input.addEventListener('input', function(){
            var matchcounts = 0;
            var str='<ul class=\"search-result-list\">';
            var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
            $resultContent.innerHTML = "";
            if (this.value.trim().length > 1) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var content_index = [];
                var data_title = data.title.trim().toLowerCase();
                var data_content = data.content.trim().replace(/<[^>]+>/g,"").toLowerCase();
                var data_url = decodeURIComponent(data.url);
                var index_title = -1;
                var index_content = -1;
                var first_occur = -1;
                // only match artiles with not empty titles and contents
                if(data_title != '') {
                  keywords.forEach(function(keyword, i) {
                    index_title = data_title.indexOf(keyword);
                    index_content = data_content.indexOf(keyword);
                    if( index_title >= 0 || index_content >= 0 ){
                      isMatch = true;
                      if (i == 0) {
                        first_occur = index_content;
                      }
                    }

                  });
                }
                // show search results
                if (isMatch) {
                  matchcounts += 1;
                  str += "<li><a href='"+ data_url +"' class='search-result-title'>"+ data_title +"</a>";
                  var content = data.content.trim().replace(/<[^>]+>/g,"");
                  if (first_occur >= 0) {
                    // cut out 100 characters
                    var start = first_occur - 20;
                    var end = first_occur + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if(start == 0){
                      end = 50;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    var match_content = content.substring(start, end);
                    // highlight all keywords
                    keywords.forEach(function(keyword){
                      var regS = new RegExp(keyword, "gi");
                      match_content = match_content.replace(regS, "<b class=\"search-keyword\">"+keyword+"</b>");
                    });

                    str += "<p class=\"search-result\">" + match_content +"...</p>"
                  }
                  str += "</li>";
                }
              })};
            str += "</ul>";
            if (matchcounts == 0) { str = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>' }
            if (keywords == "") { str = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>' }
            $resultContent.innerHTML = str;
          });
          proceedsearch();
        }
      });}

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched == false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(function(e){
      $('.popup').hide();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    });
    $('.popup').click(function(e){
      e.stopPropagation();
    });
  </script><!-- hexo-inject:begin --><!-- hexo-inject:end -->


  

  

  

  


  

</body>
</html>
